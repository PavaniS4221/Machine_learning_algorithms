# One-Hot Encoding: Mathematical Explanation

One-Hot Encoding is a technique to convert categorical variables into numerical form so that machine learning models can process them without introducing false relationships between categories.

---

## 1. Mathematical Definition

Let a categorical variable **X** have **k distinct categories**:

X ∈ {c1, c2, ..., ck}

One-Hot Encoding defines a mapping:

f: X → ℝ^k

Each category ci is mapped to a **k-dimensional binary vector** (standard basis vector):

f(ci) = ei = (0, 0, ..., 1, ..., 0)

where only the i-th position is 1 and all others are 0.

---

## 2. Example

For the feature `Color ∈ {Red, Green, Blue}`, the one-hot encodings are:

| Color | One-Hot Vector |
|-------|----------------|
| Red   | (1, 0, 0)      |
| Green | (0, 1, 0)      |
| Blue  | (0, 0, 1)      |

---

## 3. Geometric Interpretation

- Each one-hot vector lies along a unique axis in a k-dimensional space.  
- For any two different categories i ≠ j:

ei · ej = 0

This means vectors are **orthogonal**, ensuring:

- No ordering between categories  
- Equal distance between all categories

Euclidean distance between any two categories:

||ei - ej|| = √2

---

## 4. Why One-Hot Encoding is Needed

Label encoding assigns arbitrary numerical order (e.g., Red = 0, Green = 1, Blue = 2), introducing a **false ordinal relationship**.  
One-Hot Encoding avoids this by representing each category independently.

---

## 5. Effect on Linear Models

For linear and logistic regression:

y = β0 + β1x1 + β2x2 + ... + βk*xk

Each one-hot feature has its own coefficient, allowing the model to learn the **independent contribution** of each category.

---

## 6. Dummy Variable Trap

If all k one-hot columns are used:
x1 + x2 + ... + xk = 1

Causes **perfect multicollinearity**.  
**Solution:** Drop one column → k−1 features; the dropped category becomes the **baseline**.

---

## 7. Key Advantages

- Prevents false ordering of categories  
- Treats all categories equally  
- Works well with linear and distance-based models  

---

## 8. Limitation

- Increases dimensionality when categories are many  

---

## 9. Summary

One-Hot Encoding mathematically maps categorical values to **orthogonal basis vectors** in a k-dimensional space, ensuring unbiased, order-free representation of categorical data for machine learning models.
